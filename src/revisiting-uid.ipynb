{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIIrgJP3jX9V"
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "from rpy2.robjects import r, pandas2ri\n",
    "pandas2ri.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from transformers import BertTokenizerFast, BertModel, BertForMaskedLM\n",
    "from transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re, os\n",
    "import bisect\n",
    "import kenlm\n",
    "import mosestokenizer\n",
    "from string import punctuation\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "puOKL93ipFNH",
    "outputId": "343dbcd3-d42a-4518-eee9-0c0ac2f6366f"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "install.packages('lme4')\n",
    "install.packages('lmerTest')\n",
    "install.packages('ggplot2')\n",
    "install.packages('perm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LM Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262,
     "referenced_widgets": [
      "360659bccda142909965ec6e5d6b0e2f",
      "114a878aff67482a928ce41ed29ac028",
      "2eb20e4e14a44cd8be52c7adffbb0086",
      "60f907d4519047cc801eda211c19680e",
      "9f7ab2972ed74dd6b69a3d20f7b64901",
      "6d33b55928d847498bfa37b57c9f7330",
      "c126e7f5f5994baca78cfb6005363c03",
      "d8c95a40f2e04e6f8219f90062280e31",
      "4b2d902b773848b4a2be01bc3633b2f7",
      "b2cd7eb599be47158b068f28fd1b7294",
      "eccca372ba574f24bf90f1019d19cdb9",
      "6c35185ee9b24db7833393e2d83dce6e",
      "8e68b61b6e324991980ab64362a5b13b",
      "15efe74b0da049eb93d66d84f10b3092",
      "8223cacfa0814690913d5936d34b8214",
      "88f1c33ee81849ba9000282b23b7efb6",
      "f37c4b80c1ca4c46a7d4362d7472f275",
      "13cd18c98bf7491a9a27a7c3ef6aa4ae",
      "20b47415dfb34ab08ecbf7ea37619672",
      "3f7b744851d44d339f81911f9f123e5e",
      "5ee8084f887d44a6a09c48c35e9e1e4d",
      "e4b5f1da26144b2fb2cb658986b38304",
      "5f16b8940b0741fd9e4b84d9574a46ec",
      "66f6a68e1a98471595c64c64cb2184a6",
      "9ba6b8d763fd4f50912d973e4b28caf6",
      "297102f4a9a34e038d37313bbb6aace2",
      "80a0b1302cda41958c0da510d2bb0982",
      "a8a13ec26f52454295d1ef8cd51154a2",
      "466c7761b8634e95999e234cdb31cd07",
      "0ede6d1e941341c59e0691f859271da3",
      "887ed81860494f85877ca0a4a669072e",
      "f4976f17821f47bb84efb356d51fe8cc",
      "a3c6e47cf0d9474aa7e9b83786d4c859",
      "f1deda43831148c6a3fe176e4d658012",
      "973d00e6981d45b8a419e063e66b634f",
      "167e54144ee14de98628a2394e8e9b67",
      "c944a5aed5ff45a1974109645a6a2ca8",
      "878f586819f94f1db5ee4e293a483a4c",
      "662bdf8ab41647638c847dd063a9ad8c",
      "47ce7046050747ccb4dc3015677d11e8"
     ]
    },
    "id": "l1X-5O0BulIK",
    "outputId": "f60cd913-85ef-474f-e145-1d3a83a2c5f0"
   },
   "outputs": [],
   "source": [
    "STRIDE = 200\n",
    "def score_gpt(sentence, model, tokenizer, BOS=True):\n",
    "      with torch.no_grad():\n",
    "        all_log_probs = torch.tensor([], device=model.device)\n",
    "        offset_mapping = []\n",
    "        start_ind = 0\n",
    "\n",
    "        while True:\n",
    "            encodings = tokenizer(sentence[start_ind:], max_length=1022, truncation=True, return_offsets_mapping=True)\n",
    "            if BOS:\n",
    "                tensor_input = torch.tensor([[tokenizer.bos_token_id] + encodings['input_ids'] + [tokenizer.eos_token_id]], device=model.device)\n",
    "            else:                \n",
    "                tensor_input = torch.tensor([encodings['input_ids'] + [tokenizer.eos_token_id]], device=model.device)\n",
    "            output = model(tensor_input, labels=tensor_input)\n",
    "            shift_logits = output['logits'][..., :-1, :].contiguous()\n",
    "            shift_labels = tensor_input[..., 1:].contiguous()\n",
    "            log_probs = torch.nn.functional.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1), reduction='none')\n",
    "            assert torch.isclose(torch.exp(sum(log_probs)/len(log_probs)),torch.exp(output['loss']))\n",
    "            offset = 0 if start_ind == 0 else STRIDE-1\n",
    "            all_log_probs = torch.cat([all_log_probs,log_probs[offset:-1]])\n",
    "            offset_mapping.extend([(i+start_ind, j+start_ind) for i,j in encodings['offset_mapping'][offset:]])\n",
    "            if encodings['offset_mapping'][-1][1] + start_ind == len(sentence):\n",
    "                break\n",
    "            start_ind += encodings['offset_mapping'][-STRIDE][1]\n",
    "        return np.asarray(all_log_probs.cpu()), offset_mapping\n",
    "\n",
    "def score_bert(sentence, model, tokenizer):\n",
    "    mask_id = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "    with torch.no_grad():\n",
    "        all_log_probs = []\n",
    "        offset_mapping = []\n",
    "        start_ind = 0\n",
    "        while True:\n",
    "            encodings = tokenizer(sentence[start_ind:], max_length=500, truncation=True, return_offsets_mapping=True)\n",
    "            tensor_input = torch.tensor([encodings['input_ids']], device=model.device)\n",
    "            mask_input = tensor_input.clone()\n",
    "            offset = 1 if start_ind == 0 else STRIDE\n",
    "            while offset_mapping and encodings['offset_mapping'][offset][0] + start_ind > offset_mapping[-1][1] + 1:\n",
    "                offset -= 1\n",
    "            for i, word in enumerate(encodings['input_ids'][:-1]):\n",
    "                if i < offset:\n",
    "                    continue\n",
    "                mask_input[:,i]=mask_id\n",
    "                output = model(mask_input, labels=tensor_input)\n",
    "                log_probs = torch.nn.functional.log_softmax(output['logits'][:,i], dim=-1).squeeze(0)\n",
    "                all_log_probs.append(-log_probs[tensor_input[0,i]].item())\n",
    "                mask_input[:,i] = word\n",
    "            offset_mapping.extend([(i+start_ind, j+start_ind) for i,j in encodings['offset_mapping'][offset:-1]])\n",
    "            if encodings['offset_mapping'][-2][1] + start_ind >= (len(sentence)-1):\n",
    "                break\n",
    "            start_ind += encodings['offset_mapping'][-STRIDE-1][1]\n",
    "            \n",
    "        return all_log_probs, offset_mapping\n",
    "\n",
    "def score_transxl(sentence, model, tokenizer):\n",
    "    def create_offset_mapping(sentence, tokens):\n",
    "        start_ind = 0\n",
    "        mapping = []\n",
    "        for i,t in enumerate(tokens):\n",
    "            # finding delimiters for <unk> tokens\n",
    "            if t == '<unk>':\n",
    "                while sentence[start_ind].isspace():\n",
    "                    start_ind += 1\n",
    "                if i == len(tokens) - 1:\n",
    "                    mapping.append((start_ind, len(sentence)))\n",
    "                    continue\n",
    "                next_ind = 0\n",
    "                while not sentence[start_ind + next_ind].isspace() and \\\n",
    "                        start_ind + next_ind < len(sentence) and  \\\n",
    "                        (tokens[i+1].strip('@')[0].isalpha() or\n",
    "                        sentence[start_ind + next_ind:].find(tokens[i+1].strip('@')) != 0):\n",
    "                    next_ind += 1\n",
    "                mapping.append((start_ind, start_ind+next_ind))\n",
    "                start_ind += next_ind\n",
    "                continue\n",
    "\n",
    "            t = t.strip('@')\n",
    "            next_ind = sentence[start_ind:].find(t)\n",
    "            if next_ind == -1:\n",
    "                print(\"Error processing sentence...\")\n",
    "                print(t, sentence)\n",
    "                mapping.append((start_ind, len(sentence)))\n",
    "                return mapping\n",
    "            mapping.append((next_ind+start_ind, next_ind+start_ind + len(t)))\n",
    "            start_ind += next_ind + len(t)\n",
    "        return mapping\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encodings = tokenizer(sentence)\n",
    "        tensor_input = torch.tensor([[tokenizer.eos_token_id] + encodings.input_ids], device=model.device)\n",
    "        output = model(tensor_input[:,:-1])\n",
    "        log_probs = output.prediction_scores.squeeze(0)\n",
    "        target_log_probs = np.asarray(-log_probs.gather(1, tensor_input[:,1:].T).squeeze(-1).cpu())        \n",
    "        offset_mapping = create_offset_mapping(sentence, tokenizer.convert_ids_to_tokens(encodings.input_ids) )\n",
    "        return target_log_probs, offset_mapping\n",
    "    \n",
    "    \n",
    "MOSESTOKENIZER = mosestokenizer.MosesTokenizer(\"en\")\n",
    "MOSESDETOKENIZER = mosestokenizer.MosesDetokenizer(\"en\")\n",
    "MOSESNORMALIZER = mosestokenizer.MosesPunctuationNormalizer(\"en\")\n",
    "def score_ngram(sentence, model, oov_nan=False):\n",
    "    # put into wikitext-103 format\n",
    "    # return strange characters back to original form\n",
    "    tokens = [MOSESDETOKENIZER([t]) for t in MOSESTOKENIZER(sentence)]\n",
    "    tokenized_sentence = \" \".join(tokens)\n",
    "    spans = []\n",
    "    word_start = 0\n",
    "    for t in tokens:\n",
    "        while sentence[word_start] != t[0]:\n",
    "            word_start += 1\n",
    "        spans.append((word_start, word_start+len(t)))\n",
    "        word_start += len(t)\n",
    "    scores = model.full_scores(tokenized_sentence, eos=False, bos=True)\n",
    "    base_change = np.log10(np.exp(1))\n",
    "    if oov_nan:\n",
    "        return np.array([-s[0]/base_change if not s[2] else np.nan for s in scores]), spans\n",
    "    return np.array([-s[0]/base_change for s in scores]), spans\n",
    "\n",
    "def get_corpus_mean(filename, model_name, n=10000):\n",
    "    with open(filename, 'r') as f:\n",
    "        probs = []\n",
    "        for i, sentence in enumerate(f):\n",
    "            if i == n:\n",
    "                break\n",
    "            if sentence.isspace():\n",
    "                continue\n",
    "            probs.extend(score(sentence.strip(), model_name)[0])\n",
    "        return np.mean(probs)\n",
    "#get_corpus_mean('wikitext-103/wiki.train.tokens', \"bert\", n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model, tokenizer = None, None\n",
    "CORPUS_MEAN = np.nan\n",
    "import gc  \n",
    "def score(sentence, model_name):\n",
    "    global model\n",
    "    global tokenizer\n",
    "    global CORPUS_MEAN\n",
    "    def clear_cache():\n",
    "        global model\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    if \"ngram\" in model_name:\n",
    "        if type(model) != kenlm.Model:\n",
    "            clear_cache()\n",
    "            #estimated from wikitext-103 using \"get_corpus_mean\"\n",
    "            CORPUS_MEAN = 2.7068\n",
    "            model = kenlm.Model('wiki.arpa')\n",
    "        if model_name == \"ngram_oov_nan\":\n",
    "            return score_ngram(sentence, model, oov_nan=True)\n",
    "        return score_ngram(sentence, model)\n",
    "    if model_name == \"bert\":\n",
    "        if type(model) != BertForMaskedLM:\n",
    "            clear_cache()\n",
    "            #estimated from wikitext-103\n",
    "            CORPUS_MEAN = 1.4463\n",
    "            model = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "            model.eval()\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "            tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "        return score_bert(sentence, model, tokenizer)\n",
    "    if model_name == \"transxl\":\n",
    "        if type(model) != TransfoXLLMHeadModel:\n",
    "            clear_cache()\n",
    "            #estimated from wikitext-103\n",
    "            CORPUS_MEAN = 3.7033\n",
    "            model = TransfoXLLMHeadModel.from_pretrained('transfo-xl-wt103')\n",
    "            model.eval()\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "            tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\n",
    "        return score_transxl(sentence, model, tokenizer)\n",
    "    if model_name == \"dutch_gpt\":\n",
    "        if type(model) != GPT2LMHeadModel or model.config._name_or_path == 'GroNLP/gpt2-small-dutch':\n",
    "            clear_cache()\n",
    "            #estimated from wikitext-103\n",
    "            CORPUS_MEAN = 3.9453\n",
    "            model = GPT2LMHeadModel.from_pretrained(\"GroNLP/gpt2-small-dutch\")\n",
    "            model.eval()\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "            tokenizer = GPT2TokenizerFast.from_pretrained(\"GroNLP/gpt2-small-dutch\")\n",
    "        return score_gpt(sentence, model, tokenizer)\n",
    "    else: \n",
    "        if type(model) != GPT2LMHeadModel or model.config._name_or_path != 'gpt2':\n",
    "            clear_cache()\n",
    "            #estimated from wikitext-103\n",
    "            CORPUS_MEAN = 3.8845\n",
    "            model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "            model.eval()\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "            tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "        return score_gpt(sentence, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zeNipijevY1A",
    "outputId": "5d17467e-3327-40c2-f985-4844b2f1ecd6"
   },
   "outputs": [],
   "source": [
    "### Sanity check for above function\n",
    "a=['there is a pen on the desk. (HE)'*20,\n",
    "                'there is a plane on the desk',\n",
    "                        'there is a books in the desk']\n",
    "scores = [score(i, \"transxl\") for i in a]\n",
    "print([sum(s[0]) for s in scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build unigram model from wikitext-103\n",
    "def get_ngrams(sentence, n=3):\n",
    "    words = sentence.split()\n",
    "    words = [\"BOS\"]*(n-1) + words + [\"EOS\"]\n",
    "    ngrams = []\n",
    "    for i in range(len(words)-n+1):\n",
    "        ngrams.append((tuple(words[i:i+n-1]), words[i+n-1]))\n",
    "    return ngrams\n",
    "\n",
    "def normalize(root, log=True):\n",
    "    for prefix in root.keys():\n",
    "        counts = root[prefix]\n",
    "        total_counts = np.sum([counts[word] for word in counts.keys()])\n",
    "        if log:\n",
    "            root[prefix] = {k: np.log(v)- np.log(total_counts) for (k,v) in counts.items()}\n",
    "        else:\n",
    "            root[prefix] = {k: v/total_counts  for (k,v) in counts.items()}\n",
    "    return root\n",
    "\n",
    "def sampling_format(root, normalize=True):\n",
    "    for prefix in root.keys():\n",
    "        v = root[prefix]\n",
    "        words = list(v.keys())\n",
    "        counts = np.array([v[word] for word in words])\n",
    "        if normalize:\n",
    "            counts = counts/np.sum(counts)\n",
    "        root[prefix] = (words, np.log(counts))\n",
    "    return root\n",
    "\n",
    "def create_ngram_model(filename, n, outfile):\n",
    "    with open(filename, 'r') as f:\n",
    "        root = defaultdict(lambda: defaultdict(int))\n",
    "        for sentence in f:\n",
    "            if not sentence:\n",
    "                continue\n",
    "            ngrams = get_ngrams(sentence.lower().strip(), n)\n",
    "            for ngram in ngrams:\n",
    "                root[ngram[0]][ngram[1]] += 1\n",
    "        root = normalize(root, log=True)\n",
    "        pickle.dump(dict(root), open(outfile, \"wb\"))\n",
    "        return root\n",
    "\n",
    "class UnigramModel:\n",
    "    lang_mapping = {\"en_base\": \"unigram.pkl\",\n",
    "                    \"nl\": \"unigram_nl.pkl\"}\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        if lang == \"en\":\n",
    "            with open('unigrams.csv', mode='r') as infile:\n",
    "                reader = csv.reader(infile)\n",
    "                self.lookup = {rows[0]:-float(rows[1]) for rows in reader}\n",
    "        else:\n",
    "            self.lookup = pickle.load(open(UnigramModel.lang_mapping[self.lang], \"rb\"))[()]\n",
    "    def __getitem__(self, key):\n",
    "        try:\n",
    "            tokens = [MOSESDETOKENIZER([t]) for t in MOSESTOKENIZER(key)]\n",
    "            return np.sum([self.lookup.get(k, np.nan) for k in tokens])\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "freq_model = None\n",
    "def frequency(word, lang=\"en\"):\n",
    "    global freq_model\n",
    "    if not freq_model or freq_model.lang != lang:\n",
    "        freq_model = UnigramModel(lang)\n",
    "    return freq_model[word]\n",
    "        \n",
    "#create_ngram_model(\"wikitext-103/wiki.train.tokens\", 1, \"unigram.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WpRxMMne7Jsg",
    "outputId": "0f895cc2-edb1-450a-edd0-24a523ce09c5"
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "import nltk\n",
    "from scipy.special import log_softmax, softmax\n",
    "POWER_RANGE = np.arange(0., 3, 0.25)\n",
    "def power(x, y): \n",
    "    if x.mask.all():\n",
    "        return np.nan\n",
    "    return np.nanmean(x**y)\n",
    "\n",
    "def ent(x):\n",
    "    mod_x = np.nan_to_num(x.data, copy=True, nan=-np.inf)\n",
    "    l_soft = log_softmax(-mod_x)\n",
    "    return -np.sum(np.exp(l_soft)*l_soft)\n",
    "\n",
    "def ent2(x):\n",
    "    return np.sum(np.exp(-x)*x)\n",
    "\n",
    "def r_ent(x, k=2):\n",
    "    mod_x = np.nan_to_num(x.data, copy=True, nan=-np.inf)\n",
    "    soft = softmax(-mod_x)\n",
    "    return 1/(1-k)*np.log(np.sum(soft**k))\n",
    "\n",
    "def r_ent2(x, k=2):\n",
    "    return 1/(1-k)*np.log(np.sum(np.exp(-x)**k))\n",
    "\n",
    "def local_diff(x):\n",
    "    d = 0\n",
    "    for i in range(len(x)-1):\n",
    "        d += abs(x[i+1]-x[i])\n",
    "    return d/len(x)\n",
    "\n",
    "def local_diff2(x):\n",
    "    d = 0\n",
    "    for i in range(len(x)-1):\n",
    "        d += (x[i+1]-x[i])**2\n",
    "    return d/len(x)\n",
    "\n",
    "def tokenize_to_sents(s):\n",
    "    sents = []\n",
    "    for sen in nltk.sent_tokenize(s):\n",
    "        #weird failure case of sentence tokenizer\n",
    "        if sen.split()[0] != \"',\":\n",
    "            sents.append(sen)\n",
    "        else:\n",
    "            sents[-1] += sen\n",
    "    return sents\n",
    "#nltk.download('punkt')\n",
    "def string_join(x, j=''):\n",
    "    return j.join(x)\n",
    "\n",
    "def ordered_string_join(x, j=''):\n",
    "    s = sorted(x, key=lambda x: x[0])\n",
    "    a,b = list(zip(*s))\n",
    "    return a, j.join(b)\n",
    "\n",
    "def get_word_mapping(words):\n",
    "    offsets = []\n",
    "    pos = 0\n",
    "    for w in words:\n",
    "        offsets.append((pos,pos+len(w)))\n",
    "        pos += len(w) + 1\n",
    "    return offsets\n",
    "\n",
    "def string_to_log_probs(string, probs, offsets):\n",
    "    words = string.split()\n",
    "    agg_log_probs = []\n",
    "    word_mapping = get_word_mapping(words)\n",
    "    cur_prob = 0\n",
    "    cur_word_ind = 0\n",
    "    last_ind = None\n",
    "    for lp, ind in zip(probs, offsets):\n",
    "        cur_prob += lp\n",
    "        start, end = ind\n",
    "        start_cur_word, end_cur_word = word_mapping[cur_word_ind]\n",
    "        if end == end_cur_word:\n",
    "            agg_log_probs.append(cur_prob)\n",
    "            cur_prob = 0\n",
    "            cur_word_ind += 1\n",
    "        assert end <= end_cur_word\n",
    "        last_ind = ind\n",
    "    return agg_log_probs\n",
    "\n",
    "def string_to_uni_log_probs(string):\n",
    "    words = [s.strip().strip(punctuation).lower() for s in string.split()]\n",
    "    return [frequency(w.strip().strip(punctuation)) for w in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_stats(stories, models=[\"gpt\",\"bert\",\"ngram\"], wiki_mean=True, split_sens=True):\n",
    "    stats = defaultdict(dict)\n",
    "    for i, s in stories:\n",
    "      # remove leading and trailing white space\n",
    "        s = s.strip()\n",
    "        stats['split_string'][i] = s.split()\n",
    "        sents = tokenize_to_sents(s) if split_sens else [s]\n",
    "        lens = [len(sen.split()) for sen in sents]\n",
    "        assert len(s.split()) == sum(lens)\n",
    "        stats['len'][i] = np.array(lens)\n",
    "        stats['sent_markers'][i] = np.cumsum(lens) \n",
    "        stats['ch_len'][i] = np.array([sum([len(ch) for ch in sen.split()]) for sen in sents])\n",
    "        stats['uni_log_probs'][i] = np.array(string_to_uni_log_probs(s))\n",
    "        for p in POWER_RANGE:\n",
    "            stats['uni_log_prob_power_'+ str(p)][i] = []\n",
    "            for j in range(len(stats['sent_markers'][i])):\n",
    "                prev = 0 if not j else stats['sent_markers'][i][j-1]\n",
    "                end = stats['sent_markers'][i][j]\n",
    "                sent_uni_log_probs = np.ma.masked_invalid(stats['uni_log_probs'][i][prev:end])\n",
    "                stats['uni_log_prob_power_'+ str(p)][i].append(power(sent_uni_log_probs, p))  \n",
    "    \n",
    "    log_prob_stats = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    for mod in models:\n",
    "        log_prob_stats['log_probs'][mod] = {i:score(s.strip(), mod) for i,s in stories}\n",
    "        log_prob_stats['uni_log_probs'] = {i:score(s.strip(), mod) for i,s in stories}\n",
    "        lang_mean = CORPUS_MEAN if wiki_mean else np.nanmean(np.concatenate([s[0] for s in log_prob_stats['log_probs'][mod].values()]))\n",
    "        print(\"Using language mean surprisal:\", lang_mean)\n",
    "        for i, s in stories:\n",
    "            log_prob_stats['agg_log_probs'][mod][i] = np.array(string_to_log_probs(s.strip(), *log_prob_stats['log_probs'][mod][i]))\n",
    "            assert len(log_prob_stats['agg_log_probs'][mod][i]) == len(stats['split_string'][i])            \n",
    "            for j in range(len(stats['sent_markers'][i])):\n",
    "                prev = 0 if not j else stats['sent_markers'][i][j-1]\n",
    "                end = stats['sent_markers'][i][j]\n",
    "                sent_log_probs = np.ma.masked_invalid(log_prob_stats['agg_log_probs'][mod][i][prev:end])\n",
    "                log_prob_stats['log_prob_variance'][mod][i].append(np.var(sent_log_probs))\n",
    "                log_prob_stats['log_prob_variance_lang'][mod][i].append(np.mean((sent_log_probs - lang_mean)**2))\n",
    "                log_prob_stats['log_prob_max'][mod][i].append(np.amax(sent_log_probs))\n",
    "                log_prob_stats['log_prob_mean'][mod][i].append(np.mean(sent_log_probs))\n",
    "                log_prob_stats['log_prob_ldiff'][mod][i].append(local_diff(sent_log_probs))\n",
    "                log_prob_stats['log_prob_ldiff2'][mod][i].append(local_diff2(sent_log_probs))\n",
    "\n",
    "                for p in POWER_RANGE:\n",
    "                    if p == 0:\n",
    "                        log_prob_stats['log_prob_power_' + str(p)][mod][i].append(power(sent_log_probs, 0))\n",
    "                        log_prob_stats['prob_power_' + str(p)][mod][i].append(power(np.exp(-sent_log_probs), 0))\n",
    "                        continue\n",
    "                    elif p < 1:\n",
    "                        func1 = lambda x: r_ent(x, p) \n",
    "                        func2 = lambda x: r_ent2(x, p)\n",
    "                    elif p == 1:\n",
    "                        func1 = lambda x: 1/ent(x) if ent(x) else 0\n",
    "                        func2 = lambda x: 1/ent2(x) if ent2(x) else 0\n",
    "                    else:\n",
    "                        func1 = lambda x: 1/r_ent(x, p) if r_ent(x, p) else 0\n",
    "                        func2 = lambda x: 1/r_ent2(x, p) if r_ent2(x, p) else 0\n",
    "                    log_prob_stats['log_prob_entropy_' + str(p)][mod][i].append(func1(sent_log_probs))\n",
    "                    log_prob_stats['log_prob_power_' + str(p)][mod][i].append(power(sent_log_probs, p))  \n",
    "                    log_prob_stats['prob_power_' + str(p)][mod][i].append(power(np.exp(-sent_log_probs), p))\n",
    "    stats.update(log_prob_stats)\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_standard_columns(df, split_strings, lang=\"en\"):\n",
    "    # ref token is sanity check. should be same as word\n",
    "    df['ref_token'] = df.apply(lambda x: split_strings[x['text_id']][x['new_ind']], axis=1)\n",
    "    df['centered_time'] = df['time'] - df.groupby(by=[\"WorkerId\"]).transform('mean')[\"time\"]\n",
    "    df['prev_word'] = df.apply(lambda x: split_strings[x['text_id']][x['new_ind']-1] if x['new_ind']-1 >= 0 else '', axis=1)\n",
    "    df['word_len'] = df.apply(lambda x: len(x['word']), axis=1)\n",
    "    df['prev_word_len'] = df.apply(lambda x: len(x['prev_word']), axis=1)\n",
    "    df['freq'] = df.apply(lambda x: frequency(x['word'].strip().strip(punctuation).lower(), lang), axis=1)\n",
    "    df['prev_freq'] = df.apply(lambda x: frequency(x['prev_word'].strip(punctuation).lower(), lang), axis=1)\n",
    "\n",
    "def nancount(x):\n",
    "    return x.isnull().sum()\n",
    "def produce_aggregate_per_subject_sentence(main_df, line_col=False):\n",
    "    aggregate_per_subject_sentence = main_df.groupby(by=[\"WorkerId\",\"text_id\", \"sentence_num\"]).agg({\"time\":[np.sum, np.mean, np.count_nonzero], \n",
    "                                                                                                               \"word_len\":[np.sum, np.mean], \n",
    "                                                                                                               \"freq\":[np.nansum, np.nanmean, nancount],\n",
    "                                                                                                                \"outlier\": np.sum}).reset_index()\n",
    "    aggregate_per_subject_sentence.columns = ['_'.join(col).strip() for col in aggregate_per_subject_sentence.columns.values]\n",
    "    if line_col:\n",
    "        aggregate_per_subject_sentence['line_breaks'] = main_df.groupby(by=[\"WorkerId\",\"text_id\", \"sentence_num\"], as_index = False).agg({line_col: lambda x:len(np.unique(x))})[line_col]\n",
    "    aggregate_per_subject_sentence['id'] = aggregate_per_subject_sentence.apply(lambda x: str(int(x['text_id_'])) +'_'+str(int(x['sentence_num_'])), axis=1)\n",
    "    return aggregate_per_subject_sentence\n",
    "    \n",
    "def produce_aggregate_per_sentence(aggregate_per_subject_sentence, remove_outliers=True):\n",
    "    aggregate_per_sentence = aggregate_per_subject_sentence.groupby(by=[\"text_id_\", \"sentence_num_\"]).agg({\"time_sum\": np.mean, \n",
    "                                                               \"time_count_nonzero\": lambda x: scipy.stats.mode(x, nan_policy='omit')[0],\n",
    "                                                               \"time_mean\": np.mean})\n",
    "    if remove_outliers:\n",
    "        aggregate_per_subject_sentence = aggregate_per_subject_sentence.loc[aggregate_per_subject_sentence.outlier_sum ==0]\n",
    "        tmp = aggregate_per_subject_sentence.groupby(by=[\"text_id_\", \"sentence_num_\"]).agg({\"time_sum\": np.mean, \n",
    "                                                               \"time_count_nonzero\": lambda x: scipy.stats.mode(x, nan_policy='omit')[0],\n",
    "                                                               \"time_mean\": np.mean})\n",
    "        aggregate_per_sentence['time_sum_NO'] = tmp['time_sum']\n",
    "        aggregate_per_sentence['time_count_nonzero_NO'] = tmp['time_count_nonzero']        \n",
    "        aggregate_per_sentence['time_mean_NO'] = tmp['time_mean']\n",
    "    aggregate_per_sentence = aggregate_per_sentence.reset_index()\n",
    "    aggregate_per_sentence['id'] = aggregate_per_sentence.apply(lambda x: str(int(x['text_id_'])) +'_'+str(int(x['sentence_num_'])), axis=1)\n",
    "    return aggregate_per_sentence\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_log_prob_columns(df, stats, model=\"gpt\", inplace=False):\n",
    "    ret_df = df if inplace else df.copy(deep=False)\n",
    "    ret_df['model'] = model\n",
    "    ret_df['log_prob'] = ret_df.apply(lambda x: stats['agg_log_probs'][model][x['text_id']][x['new_ind']], axis=1)\n",
    "    ret_df['prev_log_prob'] = ret_df.apply(lambda x: stats['agg_log_probs'][model][x['text_id']][x['new_ind']-1] if x['new_ind']-1 >= 0 else np.nan, axis=1)\n",
    "    ret_df['prev2_log_prob'] = ret_df.apply(lambda x: stats['agg_log_probs'][model][x['text_id']][x['new_ind']-2] if x['new_ind']-2 >= 0 else np.nan, axis=1)\n",
    "    ret_df['prev3_log_prob'] = ret_df.apply(lambda x: stats['agg_log_probs'][model][x['text_id']][x['new_ind']-3] if x['new_ind']-3 >= 0 else np.nan, axis=1)\n",
    "    # rolling calcs\n",
    "    ret_df['diff_par'] = ret_df.apply(lambda x: x['log_prob']-np.mean(stats['agg_log_probs'][model][x['text_id']]), axis=1)\n",
    "    ret_df['diff2_par'] = ret_df.apply(lambda x: (x['log_prob']-np.mean(stats['agg_log_probs'][model][x['text_id']]))**2, axis=1)\n",
    "    ret_df['diff_sen'] = ret_df.apply(lambda x: x['log_prob']-stats['log_prob_mean'][model][x['text_id']][x['sentence_num']], axis=1)\n",
    "    ret_df['diff2_sen'] = ret_df.apply(lambda x: (x['log_prob']-stats['log_prob_mean'][model][x['text_id']][x['sentence_num']])**2, axis=1)\n",
    "    ret_df['diff2_lang'] = ret_df.apply(lambda x: (x['log_prob']-CORPUS_MEAN)**2, axis=1)\n",
    "    ret_df['cum_average'] = ret_df.sort_values(by='new_ind').groupby(by=[\"WorkerId\",\"text_id\", \"sentence_num\"])[\"log_prob\"].transform(lambda x: x.expanding().mean().shift(1))\n",
    "    ret_df['rolling_average1'] = ret_df.sort_values(by='new_ind').groupby(by=[\"WorkerId\",\"text_id\", \"sentence_num\"])[\"log_prob\"].transform(lambda x: x.rolling(1, min_periods = 1).mean().shift(1))\n",
    "    ret_df['rolling_average2'] = ret_df.sort_values(by='new_ind').groupby(by=[\"WorkerId\",\"text_id\", \"sentence_num\"])[\"log_prob\"].transform(lambda x: x.rolling(2, min_periods = 1).mean().shift(1))\n",
    "    ret_df['rolling_average3'] = ret_df.sort_values(by='new_ind').groupby(by=[\"WorkerId\",\"text_id\", \"sentence_num\"])[\"log_prob\"].transform(lambda x: x.rolling(3, min_periods = 1).mean().shift(1))\n",
    "    ret_df['rolling_average4'] = ret_df.sort_values(by='new_ind').groupby(by=[\"WorkerId\",\"text_id\", \"sentence_num\"])[\"log_prob\"].transform(lambda x: x.rolling(4, min_periods = 1).mean().shift(1))\n",
    "    ret_df['cum_lvar'] = ret_df.apply(lambda x: (x['log_prob']-x['cum_average'])**2, axis=1)\n",
    "    ret_df['rolling_lvar1'] = ret_df.apply(lambda x: (x['log_prob']-x['rolling_average1'])**2, axis=1)\n",
    "    ret_df['rolling_lvar2'] = ret_df.apply(lambda x: (x['log_prob']-x['rolling_average2'])**2, axis=1)\n",
    "    ret_df['rolling_lvar3'] = ret_df.apply(lambda x: (x['log_prob']-x['rolling_average3'])**2, axis=1)\n",
    "    ret_df['rolling_lvar4'] = ret_df.apply(lambda x: (x['log_prob']-x['rolling_average4'])**2, axis=1)\n",
    "\n",
    "    return ret_df\n",
    "\n",
    "def add_log_prob_aggregate_cols(aggregate_per_sentence, stats, model=\"gpt\", inplace=False):\n",
    "    ret_df = aggregate_per_sentence if inplace else aggregate_per_sentence.copy()\n",
    "    ret_df['model'] = model\n",
    "    def add_model_attribute(name):\n",
    "        ret_df[name] = aggregate_per_sentence.apply(lambda x: stats[name][model][x['text_id_']][int(x['sentence_num_'])], axis=1)\n",
    "    def add_attribute(name):\n",
    "        ret_df[name] = aggregate_per_sentence.apply(lambda x: stats[name][x['text_id_']][int(x['sentence_num_'])], axis=1)    \n",
    "    attributes = ['log_prob_mean','log_prob_max','log_prob_variance', \n",
    "                  'log_prob_variance_lang', 'log_prob_ldiff', 'log_prob_ldiff2']\n",
    "    \n",
    "    for i in POWER_RANGE:\n",
    "        add_attribute('uni_log_prob_power_'+str(i))\n",
    "        if i == 0:\n",
    "            attributes.append('log_prob_power_'+str(i))\n",
    "            attributes.append('prob_power_'+str(i))\n",
    "            continue\n",
    "        attributes.extend(['log_prob_entropy_'+str(i), 'log_prob_power_'+str(i),'prob_power_'+str(i) ])\n",
    "    for a in attributes:\n",
    "        add_model_attribute(a)\n",
    "\n",
    "    ret_df['log_prob_std'] = np.sqrt(ret_df['log_prob_variance'].astype(float))\n",
    "    ret_df['len'] = ret_df.apply(lambda x: stats['len'][x['text_id_']][int(x['sentence_num_'])], axis=1)\n",
    "    ret_df['ch_len'] = ret_df.apply(lambda x: stats['ch_len'][x['text_id_']][int(x['sentence_num_'])], axis=1)\n",
    "\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(df, field='time', transform=lambda x: x):\n",
    "    from scipy.stats import zscore\n",
    "    z_scores = zscore(transform(df[field]))\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    df.loc[:,'outlier'] = abs_z_scores > 3\n",
    "    print(\"Percentage of outliers:\", sum(df['outlier'])/len(df))\n",
    "    return df\n",
    "#remove_outliers(dundee, transform=np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_dfs(main, stats, models, inplace=True, lang=\"en\"):\n",
    "    main = main if inplace else main.copy()\n",
    "    #get standard corpus statistics\n",
    "    add_standard_columns(main, stats['split_string'], lang=lang)\n",
    "    agg_per_subject_sentence = produce_aggregate_per_subject_sentence(main)\n",
    "    agg_per_sentence = produce_aggregate_per_sentence(agg_per_subject_sentence)\n",
    "    #get log_prob related corpus statistics\n",
    "    main = pd.concat(\n",
    "        [add_log_prob_columns(main, stats, model=mod, rolling_vals=False) for mod in models])\n",
    "    agg_per_subject_sentence = pd.concat(\n",
    "        [add_log_prob_aggregate_cols(agg_per_subject_sentence, stats, model=mod) for mod in models])\n",
    "    agg_per_sentence = pd.concat(\n",
    "        [add_log_prob_aggregate_cols(agg_per_sentence, stats, model=mod) for mod in models])\n",
    "    return main, agg_per_subject_sentence, agg_per_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_stats(main, subject_sen, sen, name):\n",
    "    df_name = name+'_df.pkl'\n",
    "    pickle.dump(main, open(df_name, \"wb\"))\n",
    "    ss_name = name+'_subject_sen.pkl'\n",
    "    pickle.dump(subject_sen, open(ss_name, \"wb\"))\n",
    "    sen_name = name+'_sen.pkl'\n",
    "    pickle.dump(sen, open(sen_name, \"wb\"))\n",
    "\n",
    "def load_stats(name):\n",
    "    df_name = name+'_df.pkl'\n",
    "    ss_name = name+'_subject_sen.pkl'\n",
    "    sen_name = name+'_sen.pkl'\n",
    "    return pickle.load(open(df_name, \"rb\")), pickle.load(open(ss_name, \"rb\")), pickle.load(open(sen_name, \"rb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = ['gpt']#,'transxl', 'ngram', 'bert' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2--T130v11X7"
   },
   "outputs": [],
   "source": [
    "gpt3_probs = pd.read_csv(\"https://raw.githubusercontent.com/languageMIT/naturalstories/master/probs/all_stories_gpt3.csv\")\n",
    "# To get same indexing as stories db\n",
    "gpt3_probs[\"story\"] = gpt3_probs[\"story\"] + 1\n",
    "gpt3_probs['len'] = gpt3_probs.groupby(\"story\", sort=False)['offset'].shift(periods=-1, fill_value=0) - gpt3_probs['offset'] \n",
    "gpt3_probs['new_token'] = gpt3_probs.apply(lambda x: x['token'] if x['len'] == len(x['token']) else x['token'] + ' ', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stories_df = gpt3_probs.groupby(by=[\"story\"], sort=False).agg({\"new_token\":[string_join]}).reset_index()\n",
    "stories = list(zip(stories_df['story'], stories_df['new_token', 'string_join']))\n",
    "ns_stats = corpus_stats(stories, models=MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgoj--Y6fajD"
   },
   "outputs": [],
   "source": [
    "natural_stories = pd.read_csv(\"https://raw.githubusercontent.com/languageMIT/naturalstories/master/naturalstories_RTS/processed_RTs.tsv\", sep='\\t').drop_duplicates()\n",
    "natural_stories.rename(columns = {'RT':'time', \n",
    "                                   'item': 'text_id'}, inplace = True)\n",
    "natural_stories['new_ind'] = natural_stories['zone'] - 1\n",
    "natural_stories['sentence_num'] = natural_stories.apply(lambda x: bisect.bisect(ns_stats['sent_markers'][x['text_id']], x['new_ind']), axis=1)\n",
    "natural_stories = find_outliers(natural_stories, transform=np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_stories, ns_agg_per_subject_sentence, ns_mean_per_sen = create_analysis_dfs(natural_stories, ns_stats, MODELS) \n",
    "#pickle_stats(natural_stories, ns_agg_per_subject_sentence, ns_mean_per_sen, \"ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "iEm5Q51-QCuk",
    "outputId": "8eefaea4-50e2-4340-c7cf-0076f8d5135f"
   },
   "outputs": [],
   "source": [
    "# looks like there's a small mispelling somewhere ;)\n",
    "natural_stories[natural_stories['word'] != natural_stories['ref_token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_stories, ns_agg_per_subject_sentence, ns_mean_per_sen = load_stats(\"ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1KROJj_7OGC",
    "outputId": "fb19f538-9515-485f-86fd-ed88fba19ef7"
   },
   "outputs": [],
   "source": [
    "provo = pd.read_csv('corpora/provo.csv')\n",
    "provo.rename(columns = {'IA_DWELL_TIME':'time', 'Participant_ID': 'WorkerId', 'Word':'word', \n",
    "                        \"Text_ID\":\"text_id\", \"Sentence_Number\":\"sentence_num\",\n",
    "                       \"IA_FIRST_RUN_DWELL_TIME\": 'time2', 'IA_FIRST_FIXATION_DURATION':'time3'}, inplace = True)\n",
    "provo = provo.dropna(subset=[\"Word_Number\"])\n",
    "provo = provo.astype({\"Word_Number\": 'Int64', \"sentence_num\": 'Int64'})\n",
    "provo['word'] = provo.apply(lambda x: MOSESNORMALIZER(x['word']).strip(), axis=1)\n",
    "#fixing small discrepancy\n",
    "provo.loc[provo['word'] == '0.9', 'word'] = '90%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provo_text = pd.read_csv('corpora/provo_norms.csv')[['Text_ID','Text']].drop_duplicates().sort_values(by=['Text_ID'])\n",
    "provo_text.drop(provo_text[(provo_text.Text_ID == 27) & (~provo_text.Text.str.contains(\"doesn't\", regex=False))].index, inplace=True)\n",
    "inds = provo_text.apply(lambda x: list(range(1,len(x['Text'].split())+1)), axis=1)\n",
    "inds = {i:j for i,j in zip(provo_text['Text_ID'], inds)}\n",
    "paragraphs = {i:j.replace(u\"\\uFFFD\", \"?\") for i,j in provo_text[['Text_ID','Text']].itertuples(index=False, name=None)}\n",
    "paragraphs_split = {i:[k.strip(punctuation) for k in j.lower().split()] for i,j in paragraphs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "provo_stats = corpus_stats(paragraphs.items(), models=MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provo[\"new_ind\"] = provo[\"Word_Number\"] - 2\n",
    "provo['new_ind'] = provo.apply(lambda x: x[\"new_ind\"] + paragraphs_split[x['text_id']][x[\"new_ind\"]:].index(x[\"word\"].lower().strip(punctuation)), axis=1)\n",
    "provo['sentence_num'] = provo.apply(lambda x: bisect.bisect(provo_stats['sent_markers'][x['text_id']], x['new_ind']), axis=1)\n",
    "provo = find_outliers(provo.loc[provo['time'] != 0], transform=np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provo, provo_agg_per_subject_sentence, provo_mean_per_sen = create_analysis_dfs(provo, provo_stats, MODELS) \n",
    "provo_agg_per_subject_sentence['time2_sum'] = provo.groupby(by=[\"WorkerId\",\"text_id\", \"sentence_num\", \"model\"]).agg({\"time2\":np.sum}).reset_index()['time2']\n",
    "provo_agg_per_subject_sentence['time3_sum'] = provo.groupby(by=[\"WorkerId\",\"text_id\", \"sentence_num\", \"model\"]).agg({\"time3\":np.sum}).reset_index()['time3']\n",
    "#pickle_stats(provo, provo_agg_per_subject_sentence, provo_mean_per_sen, \"provo2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provo, provo_agg_per_subject_sentence, provo_mean_per_sen = load_stats(\"provo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCL Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucl = pd.read_csv('corpora/ucl/selfpacedreading.RT.txt','\\t')\n",
    "ucl.rename(columns = {'RT':'time', 'subj_nr': 'WorkerId', \n",
    "                        \"sent_nr\":\"text_id\"}, inplace = True)\n",
    "ucl['word'] = ucl.apply(lambda x: MOSESNORMALIZER(x['word']).strip(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds, paragraphs = zip(*ucl[['text_id','word_pos','word']].drop_duplicates().dropna().groupby(by = ['text_id']).apply(lambda x: ordered_string_join(zip(x['word_pos'], x['word']), ' ')))\n",
    "ucl_stats = corpus_stats(list(enumerate(paragraphs,1)), models=MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucl['new_ind'] = ucl.apply(lambda x: inds[x['text_id']-1].index(x[\"word_pos\"]), axis=1)\n",
    "ucl['sentence_num'] = 0\n",
    "ucl = find_outliers(ucl, transform=np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucl, ucl_agg_per_subject_sentence, ucl_mean_per_sen = create_analysis_dfs(ucl, ucl_stats, MODELS) \n",
    "pickle_stats(ucl, ucl_agg_per_subject_sentence, ucl_mean_per_sen, \"ucl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucl, ucl_agg_per_subject_sentence, ucl_mean_per_sen = load_stats(\"ucl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCL Eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucl_eye = pd.read_csv('corpora/ucl/eyetracking.RT.txt','\\t')\n",
    "ucl_eye.rename(columns = {'RTfirstpass':'time', 'subj_nr': 'WorkerId', \n",
    "                        \"sent_nr\":\"text_id\"}, inplace = True)\n",
    "ucl_eye['word'] = ucl_eye.apply(lambda x: MOSESNORMALIZER(x['word']).strip(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = ucl_eye[['text_id','word_pos','word']].drop_duplicates().dropna().groupby(by = ['text_id']).apply(lambda x: ordered_string_join(zip(x['word_pos'], x['word']), ' '))\n",
    "inds, paragraphs = zip(*joined)\n",
    "ucl_eye_stats = corpus_stats(list(zip(joined.index, paragraphs)), models=MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_dict = {i: ind_set for i, ind_set in zip(joined.index, inds)}\n",
    "ucl_eye['new_ind'] = ucl_eye.apply(lambda x: inds_dict[x['text_id']].index(x[\"word_pos\"]), axis=1)\n",
    "ucl_eye['sentence_num'] = 0\n",
    "ucl_eye = find_outliers(ucl_eye.loc[ucl_eye['time'] != 0], transform=np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucl_eye, ucl_eye_agg_per_subject_sentence, ucl_eye_mean_per_sen = create_analysis_dfs(ucl_eye, ucl_eye_stats, MODELS) \n",
    "pickle_stats(ucl_eye, ucl_eye_agg_per_subject_sentence, ucl_eye_mean_per_sen, \"ucl_eye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucl_eye, ucl_eye_agg_per_subject_sentence, ucl_eye_mean_per_sen = load_stats(\"ucl_eye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dundee Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUNDEE_MODELS = ['gpt', 'ngram', 'bert' ]\n",
    "def predict_encoding(file_path, n_lines=50):\n",
    "    '''Predict a file's encoding using chardet'''\n",
    "    import chardet\n",
    "\n",
    "    # Open the file as binary data\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Join binary lines for specified number of lines\n",
    "        rawdata = b''.join([f.readline() for _ in range(n_lines)])\n",
    "\n",
    "    return chardet.detect(rawdata)['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s\\w\\d+ma2: data in fixation order\n",
    "# WNUM in eyetracking DF maps to word index in text DF\n",
    "dundeeDir = 'corpora/dundee/eye-tracking'\n",
    "fileList = [os.path.join(dundeeDir, f) for f in os.listdir(dundeeDir) if re.match(r's\\w\\d+ma2p*\\.dat', f)]\n",
    "cols = ['WorkerId', 'text_id', 'WORD','TEXT','LINE','OLEN','WLEN','XPOS','WNUM','FDUR','OBLP','WDLP','FXNO','TXFR']\n",
    "dundee = pd.DataFrame(columns = cols)\n",
    "for file in fileList:\n",
    "    temp = pd.read_csv(file, sep='\\s+', encoding='Windows-1252')\n",
    "    match = re.search(r'(s\\w)(\\d+)ma2p*\\.dat', file.split('/')[-1])\n",
    "    subjId = match.group(1)\n",
    "    text = int(match.group(2))\n",
    "    temp.insert(loc=0, column='text_id', value=text)\n",
    "    temp.insert(loc=0, column='WorkerId', value=subjId)\n",
    "    dundee = dundee.append(temp)\n",
    "dundee.rename(columns = {'FDUR':'time', 'WORD':'word', 'WNUM': 'Word_Number'}, inplace = True)\n",
    "dundee['time'] = dundee.time.astype('int64')\n",
    "dundee = dundee.reset_index().drop(columns=['index','OLEN','XPOS','OBLP','WDLP','FXNO','TXFR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dundeeDir = 'corpora/dundee/texts'\n",
    "textList = [os.path.join(dundeeDir, f) for f in os.listdir(dundeeDir) if re.match(r'tx\\d+wrdp\\.dat', f)]\n",
    "cols = ['word', 'text_id', 'screen_nr', 'line_nr', 'pos_on_line', 'serial_nr', 'initial_letter_position', 'word_len_punct', 'word_len', 'punc_code', 'n_chars_before','n_chars_after', 'Word_Number', 'local_word_freq']\n",
    "dundeeTexts = pd.DataFrame(columns = cols)\n",
    "for text in textList:\n",
    "    temp = pd.read_csv(text, sep='\\s+', names=cols, encoding='Windows-1252')\n",
    "    dundeeTexts = dundeeTexts.append(temp)\n",
    "dundee['word'] = dundee.apply(lambda x: re.sub(r\"\\s+\", ' ', MOSESNORMALIZER(x['word'].strip().replace('\"\"','\"').replace('\\n',' '))), axis=1)#dundee.apply(lambda x: MOSESNORMALIZER(x['word']).strip(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds, paragraphs = zip(*dundeeTexts[['text_id','Word_Number','word']].drop_duplicates().dropna().groupby(by = ['text_id']).apply(lambda x: ordered_string_join(zip(x['Word_Number'], x['word']), ' ')))\n",
    "dundee_stats = corpus_stats(list(enumerate(paragraphs,1)), models=DUNDEE_MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dundee = dundee.drop(dundee[dundee['word'].map(len) > 20].index)\n",
    "dundee['new_ind'] = dundee.apply(lambda x: inds[x['text_id']-1].index(x[\"Word_Number\"]), axis=1)\n",
    "dundee['sentence_num'] = dundee.apply(lambda x: bisect.bisect(dundee_stats['sent_markers'][x['text_id']], x['new_ind']), axis=1)\n",
    "#total reading time\n",
    "temp = dundee.groupby(by=[\"WorkerId\",\"text_id\", \"sentence_num\", \"new_ind\", 'word']).agg({'time': np.nansum})\n",
    "dundee = dundee.loc[dundee['time'] != 0].drop_duplicates(subset=[\"WorkerId\",\"text_id\", \"sentence_num\", \"new_ind\", 'word'])\n",
    "dundee = dundee.groupby(by=[\"WorkerId\",\"text_id\", \"sentence_num\", \"new_ind\", 'word']).agg({'time': np.sum})\n",
    "#first pass reading time\n",
    "dundee['time2'] = dundee['time']\n",
    "dundee['time'] = temp['time']\n",
    "dundee=dundee.reset_index()\n",
    "dundee = find_outliers(dundee.loc[dundee['time'] > 0], transform=np.log)\n",
    "#See Smith & Levy 2013\n",
    "dundee.loc[dundee.WorkerId=='sg','outlier'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_sum = dundee.groupby(by=[\"WorkerId\",\"text_id\", \"sentence_num\"]).agg({\"time2\":np.sum}).reset_index()['time2']\n",
    "dundee, dundee_agg_per_subject_sentence, dundee_mean_per_sen = create_analysis_dfs(dundee, dundee_stats, DUNDEE_MODELS)\n",
    "dundee_agg_per_subject_sentence['time2_sum'] = temp_sum\n",
    "#pickle_stats(dundee, dundee_agg_per_subject_sentence, dundee_mean_per_sen, \"dundee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dundee, dundee_agg_per_subject_sentence, dundee_mean_per_sen = load_stats(\"dundee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown = pd.read_csv('corpora/brown_spr.csv')\n",
    "brown = brown.drop(columns='Unnamed: 0')\n",
    "brown.rename(columns = {'subject': 'WorkerId', \n",
    "                        \"text_pos\":\"Word_Number\"}, inplace = True)\n",
    "brown['word'] = brown.apply(lambda x: MOSESNORMALIZER(x['word']).strip(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inds, paragraphs = zip(*brown[['text_id','Word_Number','word']].drop_duplicates().dropna().groupby(by = ['text_id']).apply(lambda x: ordered_string_join(zip(x['Word_Number'], x['word']), ' ')))\n",
    "brown_stats = corpus_stats(list(enumerate(paragraphs)), models=MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown['new_ind'] = brown.apply(lambda x: inds[x['text_id']].index(x[\"Word_Number\"]), axis=1)\n",
    "brown['sentence_num'] = brown.apply(lambda x: bisect.bisect(brown_stats['sent_markers'][x['text_id']], x['new_ind']), axis=1)\n",
    "brown = find_outliers(brown, transform=np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown, brown_agg_per_subject_sentence, brown_mean_per_sen = create_analysis_dfs(brown, brown_stats, MODELS) \n",
    "#pickle_stats(brown, brown_agg_per_subject_sentence, brown_mean_per_sen, \"brown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown, brown_agg_per_subject_sentence, brown_mean_per_sen = load_stats(\"brown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GECO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = pd.read_csv(\"corpora/DutchMaterials.csv\")\n",
    "texts_df[['part','sentence']] = texts_df['SENTENCE_ID'].str.split('-',expand=True)\n",
    "texts_df['sentence'] = texts_df['sentence'].astype('int32')\n",
    "idx = texts_df.groupby([\"part\",\"sentence\"])['CHRON_ID'].idxmax()\n",
    "texts_df.loc[idx, \"WORD\"] += '.'\n",
    "inds, paragraphs = zip(*texts_df.groupby(by=[\"part\"]).apply(lambda x: ordered_string_join(zip(x['CHRON_ID'], x['WORD']), ' ')))\n",
    "#texts_df = texts_df.sort_values(['CHRON_ID']).groupby(by=[\"part\"]).agg({\"WORD\":lambda x: string_join(x, j=' ')}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geco = pd.read_csv(\"corpora/L1ReadingData.csv\").rename(columns = {'WORD_TOTAL_READING_TIME':'time', \n",
    "                                                          'PP_NR': 'WorkerId',\n",
    "                                                          \"WORD_ID_WITHIN_TRIAL\":\"Word_Number\",\n",
    "                                                          \"PART\":\"text_id\",\n",
    "                                                          \"WORD\": \"word\"})\n",
    "geco['time'] = pd.to_numeric(geco['time'], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geco_stats = corpus_stats(list(enumerate(paragraphs,1)), models=['dutch_gpt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_mapping = {x['IA_ID']: x['CHRON_ID'] for i, x in texts_df.iterrows()}\n",
    "geco['new_ind'] = geco.apply(lambda x: inds[x['text_id']-1].index(word_mapping[x[\"WORD_ID\"]]) if x[\"WORD_ID\"] in word_mapping else None, axis=1)\n",
    "geco['sentence_num'] = geco.apply(lambda x: bisect.bisect(geco_stats['sent_markers'][x['text_id']], x['new_ind']), axis=1)\n",
    "geco = geco[geco['time']!= 0].dropna(subset = [\"time\", \"new_ind\"])\n",
    "geco['new_ind'] = geco['new_ind'].astype('int32')\n",
    "geco = find_outliers(geco, transform=np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geco, geco_agg_per_subject_sentence, geco_mean_per_sen = create_analysis_dfs(geco, geco_stats, ['dutch_gpt'], lang=\"nl\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geco, geco_agg_per_subject_sentence, geco_mean_per_sen = load_stats(\"geco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_freq(sen):\n",
    "    words = [MOSESDETOKENIZER([t]) for t in MOSESTOKENIZER(MOSESNORMALIZER(sen))]\n",
    "    total = [frequency(w.strip().strip(punctuation).lower(), lang=\"en2\") for w in words]\n",
    "    return np.nansum(list(filter(lambda x: x != None, total)))\n",
    "\n",
    "def add_lau_accept_measures(df):\n",
    "    df['log_freq'] = df.apply(lambda x: abs(get_sentence_freq(x['sentence'])), axis=1)\n",
    "    df['slor'] = df['log_prob_mean'] - df['log_freq']/df['len']\n",
    "    df['normlp'] = df['log_prob_mean']*df['len']/df['log_freq']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cola = pd.read_csv('corpora/cola_public/raw/in_domain_train.tsv','\\t', header=None, names=['ID','accept','NA','sentence'])\n",
    "cola = cola.drop(columns='NA')\n",
    "cola['text_id_'] = cola.index\n",
    "cola['sentence_num_'] = 0\n",
    "cola['sentence'] = cola.apply(lambda x: MOSESNORMALIZER(x['sentence']).strip(), axis=1)\n",
    "cola_stats = corpus_stats(list(enumerate(cola['sentence'])), models=MODELS, split_sens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cola = pd.concat(\n",
    "    [add_log_prob_aggregate_cols(cola, cola_stats, model=mod) for mod in MODELS])\n",
    "add_lau_accept_measures(cola)\n",
    "#pickle.dump(cola, open(\"cola.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cola = pickle.load(open(\"cola.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc = pd.read_csv('corpora/bnc.csv','\\t')\n",
    "bnc.rename(columns = {'mean_rating':'accept', 'text':'sentence', 'length':'len'}, inplace = True)\n",
    "bnc['text_id_'] = bnc.index\n",
    "bnc['sentence_num_'] = 0\n",
    "bnc['sentence'] = bnc.apply(lambda x: re.sub(r\"\\s+\", ' ', re.sub(r'[\\u4e00-\\u9fff|\\u00b0]+', '', MOSESNORMALIZER(x['sentence'].strip().replace('\"\"','\"')))), axis=1)\n",
    "#bnc_stats = corpus_stats(list(enumerate(bnc['sentence'])), models=MODELS, split_sens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc = pd.concat(\n",
    "    [add_log_prob_aggregate_cols(bnc, bnc_stats, model=mod) for mod in MODELS])\n",
    "add_lau_accept_measures(bnc)\n",
    "pickle.dump(bnc, open(\"bnc.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc = pickle.load(open(\"bnc.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc = bnc[bnc.MOP==\"MOP2\"]\n",
    "bnc.loc[bnc.accept<2.5,\"accept\"] = 0\n",
    "bnc.loc[bnc.accept>=2.5,\"accept\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_per_subject_sentence_full = pd.concat([ns_agg_per_subject_sentence.assign(dataset=\"Natural Stories\"),\n",
    "                                           provo_agg_per_subject_sentence.assign(dataset=\"Provo\"),\n",
    "                                           dundee_agg_per_subject_sentence.assign(dataset=\"Dundee\"),\n",
    "                                           brown_agg_per_subject_sentence.assign(dataset=\"Brown\"),\n",
    "                                           ucl_eye_agg_per_subject_sentence.assign(dataset=\"UCL (ET)\"),\n",
    "                                           ucl_agg_per_subject_sentence.assign(dataset=\"UCL (R)\")])\n",
    "#agg_per_subject_sentence_full = geco_agg_per_subject_sentence.assign(dataset=\"GECO\")\n",
    "agg_per_subject_sentence_full['WorkerId_'] = agg_per_subject_sentence_full['WorkerId_'].astype(str)\n",
    "try:\n",
    "    # In case columns from different dfs are of different types\n",
    "    types = agg_per_subject_sentence_full.applymap(type).apply(set)\n",
    "    cols = types[types.apply(len) > 1].index\n",
    "    agg_per_subject_sentence_full[cols] = agg_per_subject_sentence_full[cols].apply(lambda x: x.astype(np.float64), 1)\n",
    "except TypeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_per_sentence_full = pd.concat([ns_mean_per_sen.assign(dataset=\"Natural Stories\"),\n",
    "                                   provo_mean_per_sen.assign(dataset=\"Provo\"),\n",
    "                                   dundee_mean_per_sen.assign(dataset=\"Dundee\"),\n",
    "                                   brown_mean_per_sen.assign(dataset=\"Brown\"),\n",
    "                                   ucl_eye_mean_per_sen.assign(dataset=\"UCL (ET)\"),\n",
    "                                   ucl_mean_per_sen.assign(dataset=\"UCL (R)\")])\n",
    "try:\n",
    "    types = agg_per_sentence_full.applymap(type).apply(set)\n",
    "    cols = types[types.apply(len) > 1].index\n",
    "    agg_per_sentence_full[cols] = agg_per_sentence_full[cols].apply(lambda x: x.astype(np.float64), 1)\n",
    "except TypeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBSg34hkqDQH"
   },
   "outputs": [],
   "source": [
    "%R -i agg_per_subject_sentence_full\n",
    "%R -i agg_per_sentence_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptability = pd.concat([cola.drop(['ID', 'sentence'], axis=1).assign(dataset='CoLA'), \n",
    "                           bnc.drop(['MOP', 'language', 'sentence', 'rating_list'], 1).assign(dataset='BNC')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R -i acceptability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiGB0Mo3wtP9",
    "outputId": "2682d0f2-2497-4852-cbbe-a952c8d8d0d5"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(lme4)\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "library(perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "lme_cross_val <- function(form, df, d_var, num_folds=10, shuffle=FALSE){\n",
    "    if(shuffle){\n",
    "        df <- df[sample(nrow(df)),]\n",
    "    }\n",
    "    folds <- cut(seq(1,nrow(df)),breaks=num_folds,labels=FALSE)\n",
    "    estimates <- c()\n",
    "    for(i in 1:num_folds){\n",
    "        testIndexes <- which(folds==i,arr.ind=TRUE)\n",
    "        testData <- df[testIndexes,]\n",
    "        trainData <- df[-testIndexes,]\n",
    "        model <- lmer(form,  data=trainData, REML=FALSE)\n",
    "        sigma <- mean(residuals(model)^2)\n",
    "        \n",
    "        estimate <- log(dnorm(testData[[d_var]], \n",
    "                              mean=predict(model, newdata=testData, allow.new.levels=TRUE), \n",
    "                              sd=sqrt(sigma)))\n",
    "        estimates <- c(estimates, estimate)\n",
    "    }\n",
    "    estimates\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "lm_cross_val <- function(formula, df, d_var, family=gaussian, num_folds=10, shuffle=FALSE){\n",
    "    if(shuffle){\n",
    "        df <- df[sample(nrow(df)),]\n",
    "    }\n",
    "    folds <- cut(seq(1,nrow(df)),breaks=num_folds,labels=FALSE)\n",
    "    estimates <- c()\n",
    "    for(i in 1:num_folds){\n",
    "        testIndexes <- which(folds==i,arr.ind=TRUE)\n",
    "        testData <- df[testIndexes,]\n",
    "        trainData <- df[-testIndexes,]\n",
    "        model <- glm(formula, data=trainData, family=family)\n",
    "        sigma <- sigma(model)\n",
    "        if(identical(binomial, family)){\n",
    "            predictions <- predict(model, newdata=testData, type=\"response\")\n",
    "        }else{\n",
    "            predictions <- predict(model, newdata=testData)\n",
    "        }\n",
    "        estimate <- log(dnorm(testData[[d_var]], \n",
    "                              mean=predictions, \n",
    "                              sd=sigma))\n",
    "        estimates <- c(estimates, estimate)\n",
    "        \n",
    "    }\n",
    "    estimate\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMNLP Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Psychometric Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "powers_np_format <- c('0.0','0.25', '0.5' , '0.75','1.0', '1.25', '1.5' , '1.75', '2.0', '2.25', '2.5'  )\n",
    "labels <- as.numeric(powers_np_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "df_full <- filter(agg_per_subject_sentence_full, agg_per_subject_sentence_full$log_prob_mean < 15,\n",
    "                                                  agg_per_subject_sentence_full$outlier_sum==0)\n",
    "outcome <- 'time_sum'\n",
    "predictors <- list(b=c('len', 'I(len*uni_log_prob_power_1.0)*ch_len'))\n",
    "models <- c('bert','gpt','ngram', 'transxl')\n",
    "datasets <- c('Dundee','Brown','Provo','Natural Stories')\n",
    "\n",
    "dataset_func <- function(ds){\n",
    "    print(ds)\n",
    "    data_per_ds <- filter(df_full, df_full$dataset == ds)\n",
    "    if(nrow(data_per_ds) == 0){\n",
    "                return(NULL)\n",
    "    }\n",
    "    predictors_func <- function(preds){\n",
    "        other_preds <- paste0(preds, collapse=\"+\")\n",
    "        model_func <- function(model_name){\n",
    "            data <- filter(data_per_ds, data_per_ds$model == model_name)\n",
    "            if(nrow(data) == 0){\n",
    "                return(NULL)\n",
    "            }\n",
    "            set.seed(42)\n",
    "            shuffled_order <- sample(nrow(data))\n",
    "            agg_baseline <- lme_cross_val(paste0(outcome,\"~(len + 0 | WorkerId_)+\", other_preds), data[shuffled_order,],outcome)\n",
    "            power_func <- function(x){\n",
    "                pred <- paste0(\"log_prob_power_\", x,\":len\")\n",
    "                formula <- paste0(outcome, \"~ \",pred,\"+(\",pred,\"+ len + 0 | WorkerId_) +\", other_preds)\n",
    "                cv <- lme_cross_val(formula, data[shuffled_order,], outcome, num_folds=10)\n",
    "                c(mean(cv-agg_baseline, na.rm=TRUE), var(cv-agg_baseline, na.rm=TRUE)/length(cv), mean(cv, na.rm=TRUE), sum(is.na(cv)))\n",
    "            }\n",
    "            cbind(labels, as.data.frame(do.call(rbind, lapply(powers_np_format, power_func))), model_name)\n",
    "        }\n",
    "        cbind(as.data.frame(do.call(rbind, lapply(models, model_func))), other_preds)\n",
    "    }\n",
    "    print(nrow(data_per_ds))\n",
    "    cbind(as.data.frame(do.call(rbind, lapply(predictors, predictors_func))), ds)\n",
    "}\n",
    "out <- as.data.frame(do.call(rbind, lapply(datasets, dataset_func)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "df_full <- acceptability\n",
    "predictors <- list(b=c('1'))\n",
    "models <- c('bert', 'ngram', 'gpt', \"transxl\")\n",
    "datasets <- c('BNC', 'CoLA')\n",
    "\n",
    "dataset_func <- function(ds){\n",
    "    print(ds)\n",
    "    data_per_ds <- filter(df_full, df_full$dataset == ds)\n",
    "    if(nrow(data_per_ds) == 0){\n",
    "                return(NULL)\n",
    "    }\n",
    "    family <- binomial\n",
    "    predictors_func <- function(preds){\n",
    "        other_preds <- paste0(preds, collapse=\"+\")\n",
    "        model_func <- function(model_name){\n",
    "            d <- filter(data_per_ds, data_per_ds$model == model_name)\n",
    "            if(nrow(d) == 0){\n",
    "                return(NULL)\n",
    "            }\n",
    "            set.seed(42)\n",
    "            shuffled_order <- sample(nrow(d))\n",
    "            baseline <- lm_cross_val(paste(\"accept ~ \", other_preds),\n",
    "                              d[shuffled_order,], \n",
    "                              'accept', \n",
    "                              family)\n",
    "\n",
    "            power_func <- function(x){\n",
    "                    name <- paste0(\"log_prob_power_\",x, \":len\")\n",
    "                    formula <- paste0(\"accept ~ \", name,\" +\", other_preds)\n",
    "                    cv <- lm_cross_val(formula, d[shuffled_order,], 'accept', family)\n",
    "                    c(mean(cv-baseline, na.rm=TRUE), var(cv-baseline, na.rm=TRUE)/length(cv),mean(cv, na.rm=TRUE))\n",
    "                }\n",
    "            cbind(labels, as.data.frame(do.call(rbind,lapply(powers_np_format, power_func))), model_name)\n",
    "            }\n",
    "        cbind(as.data.frame(do.call(rbind, lapply(models, model_func))),other_preds)\n",
    "    }\n",
    "    cbind(as.data.frame(do.call(rbind, lapply(predictors, predictors_func))), ds)\n",
    "}\n",
    "\n",
    "out_accept <- as.data.frame(do.call(rbind, lapply(datasets, dataset_func)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "ynezLkhDGHPM",
    "outputId": "dd70f108-3d8d-424f-d411-f80993a45dd2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "ggplot(aes(x = labels, y = V1, color=model_name), data=rbind(select( out, -c('V4')), out_accept)) + \n",
    "    geom_line() +\n",
    "    geom_vline(aes(xintercept=1),linetype=2) +\n",
    "    geom_point(size=5) +\n",
    "    geom_ribbon(aes(ymin=V1-sqrt(V2), ymax=V1+sqrt(V2)), alpha = 0.2) +\n",
    "    labs(title=\"\",y=\"Per Sentence LogLik\",x=expression(italic(\"k\"))) +\n",
    "    scale_color_discrete(name = \"\", labels=c(bert='Bert',gpt='GPT-2', ngram=expression(paste(italic(\"n\"),\"-gram\")),transxl='TransXL')) +\n",
    "    facet_wrap(~ds, scales=\"free\", ncol=6) +\n",
    "    theme_minimal() +\n",
    "    theme(text=element_text(size=22,family=\"serif\"), \n",
    "         axis.text.y=element_text(size=16,family=\"serif\"),\n",
    "        axis.text.x=element_text(size=16,family=\"serif\"),\n",
    "          aspect.ratio=1.5) \n",
    "#ggsave('test.png', width = 14, height = 8, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "df_full <- acceptability\n",
    "lau_preds <- c('slor', 'normlp')\n",
    "dataset_func <- function(ds){\n",
    "    data_per_ds <- filter(df_full, df_full$dataset == ds)\n",
    "    if(nrow(data_per_ds) == 0){\n",
    "                return(NULL)\n",
    "    }\n",
    "    family <- binomial\n",
    "    predictors_func <- function(preds){\n",
    "        other_preds <- paste0(preds, collapse=\"+\")\n",
    "        model_func <- function(model_name){\n",
    "            d <- filter(data_per_ds, data_per_ds$model == model_name)\n",
    "            if(nrow(d) == 0){\n",
    "                return(NULL)\n",
    "            }\n",
    "            set.seed(42)\n",
    "            shuffled_order <- sample(nrow(d))\n",
    "            baseline <- lm_cross_val(paste(\"accept ~ \", other_preds),\n",
    "                              d[shuffled_order,], \n",
    "                              'accept', \n",
    "                              family)\n",
    "\n",
    "            inner <- function(x){\n",
    "                    formula <- paste0(\"accept ~ \", x,\" +\", other_preds)\n",
    "                    cv <- lm_cross_val(formula, d[shuffled_order,], 'accept', family)\n",
    "                    c(mean(cv-baseline, na.rm=TRUE), var(cv-baseline, na.rm=TRUE)/length(cv),mean(cv, na.rm=TRUE))\n",
    "                }\n",
    "            cbind(lau_preds, as.data.frame(do.call(rbind,lapply(lau_preds, inner))), model_name)\n",
    "            }\n",
    "        cbind(as.data.frame(do.call(rbind, lapply(models, model_func))),other_preds)\n",
    "    }\n",
    "    cbind(as.data.frame(do.call(rbind, lapply(predictors, predictors_func))), ds)\n",
    "}\n",
    "\n",
    "lau_out <- as.data.frame(do.call(rbind, lapply(datasets, dataset_func)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "ggplot(aes(x = labels, y = V1, fill=other_preds, shape=model_name ), data=out_accept) + \n",
    "    geom_line() +\n",
    "    geom_point(size=5) +\n",
    "    geom_ribbon(aes(ymin=V1-sqrt(V2), ymax=V1+sqrt(V2)), alpha = 0.2) +\n",
    "    geom_hline(aes(yintercept = V1, linetype=model_name, color=lau_preds ), data=filter(lau_out, ds != \"coa\")) +\n",
    "    ylab(\"Per Sentence LogLik\") +\n",
    "    xlab(\"k\") +\n",
    "    ggtitle(\"\") +\n",
    "    scale_shape_discrete(name = \"Model\", labels=c('Bert','5-gram','GPT-2', \"Transxl\")) +\n",
    "    theme_minimal() +\n",
    "    facet_wrap(~ds, scales=\"free\") +\n",
    "    theme(text=element_text(size=20,family=\"serif\"))\n",
    "#ggsave('enwiki_perp_pred.png', width = 8, height = 8, dpi=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R -i agg_per_subject_sentence_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['mean','max','variance', 'variance_lang', 'ldiff', 'ldiff2', 'std']\n",
    "p = [0.25, 1.0, 1.25, 1.5, 1.75, 2.0]\n",
    "for i in p:\n",
    "    attributes.extend(['entropy_'+str(i), 'power_'+str(i)])\n",
    "attributes = sorted(attributes)\n",
    "%R -i attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "aggregate_per_sentence <- filter(agg_per_subject_sentence_full, \n",
    "                                 agg_per_subject_sentence_full$model == 'gpt', \n",
    "                                 agg_per_subject_sentence_full$log_prob_mean < 15,\n",
    "                                agg_per_subject_sentence_full$outlier_sum==0)\n",
    "datasets <- c('Dundee','Brown','Provo','Natural Stories')\n",
    "#datasets <- c('CoLA','BNC')\n",
    "df <- c()\n",
    "names <- c()\n",
    "for(d in datasets){\n",
    "    print(d)\n",
    "    names <- c(names, paste0(d, \"_mean\"), paste0(d,\"_var\"))\n",
    "    set.seed(42)\n",
    "    data <- filter(aggregate_per_sentence, dataset == d)\n",
    "    shuffled_order <- sample(nrow(data))\n",
    "    baseline <- lme_cross_val(\"time_sum ~   time_count_nonzero +len + I(len*uni_log_prob_power_1.0)*ch_len + (  len+0 | WorkerId_) \", \n",
    "                              data[shuffled_order,],\n",
    "                             'time_sum')\n",
    "    #baseline <- lm_cross_val(\"accept~1\", data[shuffled_order,], 'accept', binomial)\n",
    "    out1 <- list()\n",
    "    out1['var'] <- c()\n",
    "    out1['mean'] <- c()\n",
    "    for(v in attributes){\n",
    "        pred <- paste0(\"log_prob_\",v,\":len \")\n",
    "        formula <- paste0(\"time_sum ~ \",pred,\" + time_count_nonzero+len +I(len*uni_log_prob_power_1.0)*ch_len+ (\",pred,\" +len+0 | WorkerId_) \")\n",
    "        diff <- lme_cross_val(formula, data[shuffled_order,], 'time_sum') - baseline\n",
    "        #diff <- lm_cross_val(formula, data[shuffled_order,], 'accept', binomial) - baseline\n",
    "        out1[['var']] <- c(out1[['var']],  var(diff, na.rm=TRUE)/length(diff))\n",
    "        out1[['mean']] <- c(out1[['mean']],  mean(diff, na.rm=TRUE))\n",
    "    }\n",
    "    \n",
    "    df <- cbind(df, out1[['mean']], out1[['var']])\n",
    "}\n",
    "colnames(df) <- names\n",
    "df <- cbind(attributes, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6raxtnAIkGSH"
   },
   "outputs": [],
   "source": [
    "# Need to do this each time you switch data sets!\n",
    "ds = \"dundee\"\n",
    "model = \"gpt\"\n",
    "data_full = dundee.drop([\"word\"], axis=1)\n",
    "data = data_full.loc[(data_full['model'] == model) & (data_full['outlier'] == False)]\n",
    "\n",
    "%R -i data\n",
    "%R -i ds\n",
    "%R -i model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "set.seed(42)\n",
    "shuffled_order <- sample(nrow(data))\n",
    "powers <- seq(0.25, 2.75, by=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "powers_np_format2 <- c('1.0', '1.25', '1.5' , '1.75', '2.0', '2.25', '2.5' )\n",
    "other_preds <- paste(c('log_prob', 'prev_log_prob', 'prev_freq*prev_word_len','freq*word_len'), collapse=\" + \")\n",
    "baseline <- lme_cross_val(paste0(\"time ~\", other_preds, \"+ (1 | WorkerId)\"), data[shuffled_order,], 'time')\n",
    "\n",
    "predictor_func <- function(name){\n",
    "        formula <- paste0(\"time ~ \",name,\"+(1 +\", name,\"| WorkerId)+\", other_preds)\n",
    "        cv <- lme_cross_val(formula, data[shuffled_order,], 'time')\n",
    "        diff <- cv-baseline\n",
    "        c(mean(diff[!is.infinite(diff)], na.rm=TRUE), var(diff[!is.infinite(diff)], na.rm=TRUE)/length(cv), mean(cv[!is.infinite(cv)], na.rm=TRUE))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "out <- list()\n",
    "out[[\"cum_lvar\"]] <- predictor_func('cum_lvar')\n",
    "out[[\"rolling_lvar1\"]] <- predictor_func('rolling_lvar1')\n",
    "out[[\"rolling_lvar2\"]] <- predictor_func('rolling_lvar2')\n",
    "out[[\"rolling_lvar3\"]] <- predictor_func('rolling_lvar3')\n",
    "out[[\"rolling_lvar4\"]] <- predictor_func('rolling_lvar4')\n",
    "\n",
    "out[[\"diff_par\"]] <- predictor_func('diff_par')\n",
    "out[[\"diff2_par\"]] <- predictor_func('diff2_par')\n",
    "out[[\"diff_sen\"]] <- predictor_func('diff_sen')\n",
    "out[[\"diff2_sen\"]] <- predictor_func('diff2_sen')\n",
    "out[[\"diff2_lang\"]] <- predictor_func('diff2_lang')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "atts <- c(\"rolling_lvar1\", \"rolling_lvar2\", \"rolling_lvar3\", \"rolling_lvar4\", \"cum_lvar\", \"diff2_sen\",\"diff2_par\",\"diff2_lang\")\n",
    "att_names <- c(\"-1\",\"-2\",\"-3\",\"-4\",\"-n\",\"sent\",\"doc\",\"lang\")\n",
    "levels <- seq(length(atts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "all_vars <- as_tibble(rbind(cbind(do.call(rbind, out_ns2[atts]), model=\"GPT-2\", df=\"Natural\\nStories\", levels),\n",
    "                            cbind(do.call(rbind, out_provo2[atts]), model=\"GPT-2\",df=\"Provo\", levels),\n",
    "                            cbind(do.call(rbind, out_dundee2[atts]), model=\"GPT-2\",df=\"Dundee\", levels),\n",
    "                            cbind(do.call(rbind, out_brown2[atts]), model=\"GPT-2\",df=\"Brown\", levels)))\n",
    "all_vars[c('V1','V2','V3')] <- lapply(all_vars[c('V1','V2','V3')], as.numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "ggplot(aes(x = levels, y = V1, color=df), data=all_vars[all_vars$model==\"GPT-2\",]) + \n",
    "    geom_point(size=7) +\n",
    "    geom_errorbar(aes(ymin=V1-sqrt(V2), ymax=V1+sqrt(V2))) +\n",
    "    theme_minimal() +\n",
    "      labs(x = \"Window\", y=\"Per Token LogLik\", title=\"\")+\n",
    "    theme(text=element_text(size=16,family=\"serif\"), \n",
    "         title=element_text(size=20,family=\"serif\"),\n",
    "         axis.text.x = element_text(angle=45, size=14),\n",
    "         axis.text.y = element_text(size=8, angle=45),\n",
    "          axis.title.y = element_text(size=14),\n",
    "         strip.text.x = element_text(size=18),\n",
    "         aspect.ratio = 2,\n",
    "         legend.position = \"none\",\n",
    "         panel.spacing = unit(0, \"lines\")) +\n",
    "    scale_x_discrete(labels=att_names) +\n",
    "    facet_wrap(~df, scales=\"free\",ncol=4) \n",
    "#ggsave('windows_re.png', width = 9, height = 8, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model='bert'\n",
    "p = POWER_RANGE[(POWER_RANGE>0) & (POWER_RANGE<3)]\n",
    "names = ['log_prob_power_' + str(i) for i in p]\n",
    "names2 = ['slor', 'normlp']\n",
    "cola_r = cola.loc[:,names+names2+['len', 'accept','model']].copy()\n",
    "cola_r.loc[:,names+names2] = -cola_r[names+names2].multiply(cola_r['len'], axis=\"index\")\n",
    "total = cola_r.groupby('model').agg(['count']).iloc[0]\n",
    "cola_r = cola_r.groupby('model').corr().accept.reset_index()\n",
    "cola_r['se'] = np.sqrt((1-cola_r.accept**2)/(total[0]/2 - 2))\n",
    "%R -i cola_r\n",
    "%R -i names\n",
    "%R -i names2\n",
    "%R -i p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc_r = bnc.loc[:,names+names2+['len', 'accept','model']].copy()\n",
    "bnc_r.loc[:,names+names2] = -bnc_r[names+names2].multiply(bnc_r['len'], axis=\"index\")\n",
    "total = bnc_r.groupby('model').agg(['count']).iloc[0]\n",
    "bnc_r = bnc_r.groupby('model').corr().accept.reset_index()\n",
    "bnc_r['se'] = np.sqrt((1-cola_r.accept**2)/(total[0]/2 - 2))\n",
    "%R -i bnc_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "corrs <- rbind(cbind(filter(cola_r, cola_r$level_1 %in% unlist(names)),df=\"CoLA\",p=rep(p,length(unique(cola_r$model)))),\n",
    "              cbind(filter(bnc_r, bnc_r$level_1 %in% unlist(names)),df=\"BNC\",p=rep(p,length(unique(bnc_r$model)))))\n",
    "corrs_base <- rbind(cbind(filter(cola_r, cola_r$level_1 %in% unlist(names2)),df=\"CoLA\"),\n",
    "              cbind(filter(bnc_r, bnc_r$level_1 %in% unlist(names2)),df=\"BNC\"))\n",
    "\n",
    "ggplot(aes(x = p, y = accept, color=model, fill=model), data=corrs) + \n",
    "    geom_vline(aes(xintercept = 1),linetype=2) +\n",
    "    geom_line() +\n",
    "    geom_point(size=5) +\n",
    "    geom_ribbon(aes(ymin=accept-se, ymax=accept+se), alpha = 0.2) +\n",
    "    geom_hline(aes(yintercept = as.numeric(accept), color=model, linetype=level_1), data=corrs_base) +\n",
    "    theme_minimal() +\n",
    "    scale_linetype_discrete(name = \"\", labels=c('NormLP', 'SLOR'))+\n",
    "    scale_color_discrete(name = \"Model\", labels=c('Bert','GPT-2', 'n-gram','TransXL')) +\n",
    "    scale_fill_discrete(name = \"Model\", labels=c('Bert','GPT-2', 'n-gram','TransXL')) +\n",
    "    labs(x = \"Exponent\", y=\"Pearson's Correlation\", title=\"Surprisal-Acceptability Correlation\")+\n",
    "    theme(text=element_text(size=26,family=\"serif\"), \n",
    "         title=element_text(size=23,family=\"serif\"),\n",
    "          aspect.ratio=1.5) +\n",
    "    facet_wrap(~df, scales=\"free\") + \n",
    "    xlim(0.25,2.75)\n",
    "#ggsave('front_alt.png', width = 8, height = 8, dpi=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "a3KD3WXU3l-O",
    "1r_n9OWV3l-Q",
    "JEA1ju653l-p",
    "q-EIELH43l_T"
   ],
   "name": "language_modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ede6d1e941341c59e0691f859271da3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "114a878aff67482a928ce41ed29ac028": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13cd18c98bf7491a9a27a7c3ef6aa4ae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15efe74b0da049eb93d66d84f10b3092": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "167e54144ee14de98628a2394e8e9b67": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47ce7046050747ccb4dc3015677d11e8",
      "placeholder": "",
      "style": "IPY_MODEL_662bdf8ab41647638c847dd063a9ad8c",
      "value": " 1.36M/1.36M [00:01&lt;00:00, 828kB/s]"
     }
    },
    "20b47415dfb34ab08ecbf7ea37619672": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4b5f1da26144b2fb2cb658986b38304",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5ee8084f887d44a6a09c48c35e9e1e4d",
      "value": 1042301
     }
    },
    "297102f4a9a34e038d37313bbb6aace2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2eb20e4e14a44cd8be52c7adffbb0086": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d33b55928d847498bfa37b57c9f7330",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9f7ab2972ed74dd6b69a3d20f7b64901",
      "value": 665
     }
    },
    "360659bccda142909965ec6e5d6b0e2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2eb20e4e14a44cd8be52c7adffbb0086",
       "IPY_MODEL_60f907d4519047cc801eda211c19680e"
      ],
      "layout": "IPY_MODEL_114a878aff67482a928ce41ed29ac028"
     }
    },
    "3f7b744851d44d339f81911f9f123e5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66f6a68e1a98471595c64c64cb2184a6",
      "placeholder": "",
      "style": "IPY_MODEL_5f16b8940b0741fd9e4b84d9574a46ec",
      "value": " 1.04M/1.04M [00:02&lt;00:00, 502kB/s]"
     }
    },
    "466c7761b8634e95999e234cdb31cd07": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "47ce7046050747ccb4dc3015677d11e8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b2d902b773848b4a2be01bc3633b2f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eccca372ba574f24bf90f1019d19cdb9",
       "IPY_MODEL_6c35185ee9b24db7833393e2d83dce6e"
      ],
      "layout": "IPY_MODEL_b2cd7eb599be47158b068f28fd1b7294"
     }
    },
    "5ee8084f887d44a6a09c48c35e9e1e4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5f16b8940b0741fd9e4b84d9574a46ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60f907d4519047cc801eda211c19680e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8c95a40f2e04e6f8219f90062280e31",
      "placeholder": "",
      "style": "IPY_MODEL_c126e7f5f5994baca78cfb6005363c03",
      "value": " 665/665 [00:20&lt;00:00, 32.0B/s]"
     }
    },
    "662bdf8ab41647638c847dd063a9ad8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66f6a68e1a98471595c64c64cb2184a6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c35185ee9b24db7833393e2d83dce6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88f1c33ee81849ba9000282b23b7efb6",
      "placeholder": "",
      "style": "IPY_MODEL_8223cacfa0814690913d5936d34b8214",
      "value": " 548M/548M [00:20&lt;00:00, 26.8MB/s]"
     }
    },
    "6d33b55928d847498bfa37b57c9f7330": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80a0b1302cda41958c0da510d2bb0982": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ede6d1e941341c59e0691f859271da3",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_466c7761b8634e95999e234cdb31cd07",
      "value": 456318
     }
    },
    "8223cacfa0814690913d5936d34b8214": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "878f586819f94f1db5ee4e293a483a4c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "887ed81860494f85877ca0a4a669072e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88f1c33ee81849ba9000282b23b7efb6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e68b61b6e324991980ab64362a5b13b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "973d00e6981d45b8a419e063e66b634f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_878f586819f94f1db5ee4e293a483a4c",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c944a5aed5ff45a1974109645a6a2ca8",
      "value": 1355256
     }
    },
    "9ba6b8d763fd4f50912d973e4b28caf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_80a0b1302cda41958c0da510d2bb0982",
       "IPY_MODEL_a8a13ec26f52454295d1ef8cd51154a2"
      ],
      "layout": "IPY_MODEL_297102f4a9a34e038d37313bbb6aace2"
     }
    },
    "9f7ab2972ed74dd6b69a3d20f7b64901": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a3c6e47cf0d9474aa7e9b83786d4c859": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_973d00e6981d45b8a419e063e66b634f",
       "IPY_MODEL_167e54144ee14de98628a2394e8e9b67"
      ],
      "layout": "IPY_MODEL_f1deda43831148c6a3fe176e4d658012"
     }
    },
    "a8a13ec26f52454295d1ef8cd51154a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4976f17821f47bb84efb356d51fe8cc",
      "placeholder": "",
      "style": "IPY_MODEL_887ed81860494f85877ca0a4a669072e",
      "value": " 456k/456k [00:01&lt;00:00, 455kB/s]"
     }
    },
    "b2cd7eb599be47158b068f28fd1b7294": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c126e7f5f5994baca78cfb6005363c03": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c944a5aed5ff45a1974109645a6a2ca8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d8c95a40f2e04e6f8219f90062280e31": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4b5f1da26144b2fb2cb658986b38304": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eccca372ba574f24bf90f1019d19cdb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15efe74b0da049eb93d66d84f10b3092",
      "max": 548118077,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8e68b61b6e324991980ab64362a5b13b",
      "value": 548118077
     }
    },
    "f1deda43831148c6a3fe176e4d658012": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f37c4b80c1ca4c46a7d4362d7472f275": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_20b47415dfb34ab08ecbf7ea37619672",
       "IPY_MODEL_3f7b744851d44d339f81911f9f123e5e"
      ],
      "layout": "IPY_MODEL_13cd18c98bf7491a9a27a7c3ef6aa4ae"
     }
    },
    "f4976f17821f47bb84efb356d51fe8cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
